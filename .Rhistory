tweet.content <- Tweets %>%
select(Date, text, emotion, sentiment, score) %>%
group_by(Date, sentiment, text, emotion) %>%
summarise(sentiment_score = sum(score)) %>%
rename(Tweet = text)
# Libraries ####
library(tidyverse)
library(splitstackshape)
library(tm)
library(tidymodels)
library(tidytext)
library(textrecipes)
# library(ggwordcloud)
library(textdata)
library(saotd)
library(syuzhet)
library(stringr)
library(lubridate)
library(DT)
library(ggiraph)
# library(plotly)
# library(showtext)
#
# # library(extrafont)
# # font_import("/Users/andrewmolo/Library/Fonts")
# # fonts()
# # fonttable()
#
# font_add_google('Roboto Condensed', 'Roboto Condensed')
# showtext_auto()
# Data ####
Data <- read.csv("Data/Tweets2.csv")
RawCopy <- Data
# Data wrangling ####
# Split the column user_location to Location
Data <- cSplit(Data, 'user_location', sep=",", type.convert=FALSE)
# Convert Date column from character to Date
Data$date <- as.Date(Data$date, "%d/%m/%Y")
# Remove emoticons, punctuation marks, and stop words from the DF
TidyTweets <- saotd::tweet_tidy(DataFrame = Data)
# Overall DF with Sentiment classification
TidyTweets <- tibble::rowid_to_column(TidyTweets, "id")
senti.score <- data_frame(id=TidyTweets$id, text = TidyTweets$text, loc=TidyTweets$user_location_1,  datee = TidyTweets$date) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
inner_join(get_sentiments("nrc")) %>%
mutate(score = ifelse(sentiment=='positive',1,ifelse(sentiment=='joy',1,ifelse(sentiment=='anticipation',1,ifelse(sentiment=='trust',1,ifelse(sentiment=='surprise',1,-1)))))) %>%
mutate(emotion = sentiment) %>%
group_by(id, datee, loc, emotion) %>%
summarise(total_score = sum(score)) %>%
mutate(sentiment = ifelse(total_score>0,'Positive',ifelse(total_score<0,'Negative','Neutral')))
# Get the  which contains tweet content & sentiment
senti.score <- TidyTweets %>%
inner_join(senti.score, by='id') %>%
dplyr::select('id', 'text','sentiment','datee','loc', 'total_score', 'emotion')
# Recode the response variable to integers & rename some columns
Tweets <- senti.score %>%
mutate(score = recode(sentiment, 'Negative'=-1, 'Neutral'=0, 'Positive'=1)) %>%
rename(Date = datee,
Location=loc,
`Sentiment intensity` = total_score)
# Create a unique list of the sentiments to be used in the shiny app
sentiment_options <- unique(Tweets$sentiment)
# Data viz ####
# Line Chart: Sentiment intensity over time
time.series.tweets <- Tweets %>%
dplyr::select(Date,`Sentiment intensity`, Location, sentiment) %>%
group_by(Date, Location, sentiment) %>%
summarise(`sentiment scores` = n()) %>%
ggplot(aes(x=Date, y=`sentiment scores`, group=sentiment)) +
geom_line_interactive(aes(color=sentiment), linewidth=0.5)+
# geom_line(aes(color=sentiment), linewidth=0.5) +
theme_minimal(base_size = 11)+
scale_color_manual(values =c("#C41C0A","#3EC9CE"))+
xlab("") +
# labs(caption = "Source: Twitter") +
theme(legend.position = "top",
legend.text = element_text(family = "Roboto Condensed"),
axis.title.y =  element_text(family = "Roboto Condensed", face = "bold"),
axis.text =  element_text(size = 12, face = "bold", family = "Roboto Condensed"))+
scale_color_discrete(name=NULL)
# Column Chart: Sentiment scores per location
loc.sentiscore <- Tweets %>%
dplyr::select(Date,`Sentiment intensity`, Location, sentiment) %>%
group_by(Location, sentiment) %>%
summarise(`sentiment scores` = sum(`Sentiment intensity`)) %>%
ggplot(aes(reorder(Location, -`sentiment scores`), `sentiment scores`, fill=sentiment)) +
geom_col() +
theme_minimal() +
xlab("") +
# labs(caption = "Source: Twitter") +
theme(legend.position = "top",
legend.text = element_text(family = "Roboto Condensed"),
axis.title.y =  element_text(family = "Roboto Condensed", face = "bold"),
axis.text =  element_text(size = 12, face = "bold", family = "Roboto Condensed"))+
scale_fill_discrete(name=NULL) +
geom_text(aes(label = `sentiment scores`), vjust = -0.5, size = 3)
# Column Chart: sentiment scores per emotion
emotion.sentiscore <- Tweets %>%
dplyr::select(Date,`Sentiment intensity`, Location, sentiment, emotion) %>%
group_by(emotion, sentiment) %>%
summarise(`sentiment scores` = sum(`Sentiment intensity`)) %>%
ggplot(aes(reorder(emotion, -`sentiment scores`), `sentiment scores`, fill=sentiment)) +
geom_col() +
theme_minimal() +
xlab("") +
# labs(caption = "Source: Twitter") +
theme(legend.position = "top",
legend.text = element_text(family = "Roboto Condensed"),
axis.title.y =  element_text(family = "Roboto Condensed", face = "bold"),
axis.text =  element_text(size = 12, face = "bold", family = "Roboto Condensed"))+
scale_fill_discrete(name=NULL) +
geom_text(aes(label = `sentiment scores`), vjust = -0.5, size = 3)
## DT of tweets
tweet.content <- Tweets %>%
select(Date, text, emotion, sentiment, score) %>%
group_by(Date, sentiment, text, emotion) %>%
summarise(sentiment_score = sum(score)) %>%
rename(Tweet = text)
tweet.content <- Tweets %>%
dplyr::select(Date, text, emotion, sentiment, score) %>%
group_by(Date, sentiment, text, emotion) %>%
summarise(sentiment_score = sum(score)) %>%
rename(Tweet = text)
# Libraries ####
library(tidyverse)
library(splitstackshape)
library(tm)
library(tidymodels)
library(tidytext)
library(textrecipes)
# library(ggwordcloud)
library(textdata)
library(saotd)
library(syuzhet)
library(stringr)
library(lubridate)
library(DT)
library(ggiraph)
# library(plotly)
# library(showtext)
#
# # library(extrafont)
# # font_import("/Users/andrewmolo/Library/Fonts")
# # fonts()
# # fonttable()
#
# font_add_google('Roboto Condensed', 'Roboto Condensed')
# showtext_auto()
# Data ####
Data <- read.csv("Data/Tweets2.csv")
RawCopy <- Data
# Data wrangling ####
# Split the column user_location to Location
Data <- cSplit(Data, 'user_location', sep=",", type.convert=FALSE)
# Convert Date column from character to Date
Data$date <- as.Date(Data$date, "%d/%m/%Y")
# Remove emoticons, punctuation marks, and stop words from the DF
TidyTweets <- saotd::tweet_tidy(DataFrame = Data)
# Overall DF with Sentiment classification
TidyTweets <- tibble::rowid_to_column(TidyTweets, "id")
senti.score <- data_frame(id=TidyTweets$id, text = TidyTweets$text, loc=TidyTweets$user_location_1,  datee = TidyTweets$date) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
inner_join(get_sentiments("nrc")) %>%
mutate(score = ifelse(sentiment=='positive',1,ifelse(sentiment=='joy',1,ifelse(sentiment=='anticipation',1,ifelse(sentiment=='trust',1,ifelse(sentiment=='surprise',1,-1)))))) %>%
mutate(emotion = sentiment) %>%
group_by(id, datee, loc, emotion) %>%
summarise(total_score = sum(score)) %>%
mutate(sentiment = ifelse(total_score>0,'Positive',ifelse(total_score<0,'Negative','Neutral')))
# Get the  which contains tweet content & sentiment
senti.score <- TidyTweets %>%
inner_join(senti.score, by='id') %>%
dplyr::select('id', 'text','sentiment','datee','loc', 'total_score', 'emotion')
# Recode the response variable to integers & rename some columns
Tweets <- senti.score %>%
mutate(score = recode(sentiment, 'Negative'=-1, 'Neutral'=0, 'Positive'=1)) %>%
rename(Date = datee,
Location=loc,
`Sentiment intensity` = total_score)
# Create a unique list of the sentiments to be used in the shiny app
sentiment_options <- unique(Tweets$sentiment)
# Data viz ####
# Line Chart: Sentiment intensity over time
time.series.tweets <- Tweets %>%
dplyr::select(Date,`Sentiment intensity`, Location, sentiment) %>%
group_by(Date, Location, sentiment) %>%
summarise(`sentiment scores` = n()) %>%
ggplot(aes(x=Date, y=`sentiment scores`, group=sentiment)) +
geom_line_interactive(aes(color=sentiment), linewidth=0.5)+
# geom_line(aes(color=sentiment), linewidth=0.5) +
theme_minimal(base_size = 11)+
scale_color_manual(values =c("#C41C0A","#3EC9CE"))+
xlab("") +
# labs(caption = "Source: Twitter") +
theme(legend.position = "top",
legend.text = element_text(family = "Roboto Condensed"),
axis.title.y =  element_text(family = "Roboto Condensed", face = "bold"),
axis.text =  element_text(size = 12, face = "bold", family = "Roboto Condensed"))+
scale_color_discrete(name=NULL)
# Column Chart: Sentiment scores per location
loc.sentiscore <- Tweets %>%
dplyr::select(Date,`Sentiment intensity`, Location, sentiment) %>%
group_by(Location, sentiment) %>%
summarise(`sentiment scores` = sum(`Sentiment intensity`)) %>%
ggplot(aes(reorder(Location, -`sentiment scores`), `sentiment scores`, fill=sentiment)) +
geom_col() +
theme_minimal() +
xlab("") +
# labs(caption = "Source: Twitter") +
theme(legend.position = "top",
legend.text = element_text(family = "Roboto Condensed"),
axis.title.y =  element_text(family = "Roboto Condensed", face = "bold"),
axis.text =  element_text(size = 12, face = "bold", family = "Roboto Condensed"))+
scale_fill_discrete(name=NULL) +
geom_text(aes(label = `sentiment scores`), vjust = -0.5, size = 3)
# Column Chart: sentiment scores per emotion
emotion.sentiscore <- Tweets %>%
dplyr::select(Date,`Sentiment intensity`, Location, sentiment, emotion) %>%
group_by(emotion, sentiment) %>%
summarise(`sentiment scores` = sum(`Sentiment intensity`)) %>%
ggplot(aes(reorder(emotion, -`sentiment scores`), `sentiment scores`, fill=sentiment)) +
geom_col() +
theme_minimal() +
xlab("") +
# labs(caption = "Source: Twitter") +
theme(legend.position = "top",
legend.text = element_text(family = "Roboto Condensed"),
axis.title.y =  element_text(family = "Roboto Condensed", face = "bold"),
axis.text =  element_text(size = 12, face = "bold", family = "Roboto Condensed"))+
scale_fill_discrete(name=NULL) +
geom_text(aes(label = `sentiment scores`), vjust = -0.5, size = 3)
## DT of tweets
tweet.content <- Tweets %>%
dplyr::select(Date, text, emotion, sentiment, score) %>%
group_by(Date, sentiment, text, emotion) %>%
summarise(sentiment_score = sum(score)) %>%
rename(Tweet = text)
# Calculate the percentage and count of sentiments
table(Tweets$sentiment)
Total <-2383+764+6151
negative <- round((2383/Total)*100,0)
neutral <-  round((764/Total)*100,0)
positive <-  round((6151/Total)*100,0)
negative  <- c("26% (2,380)")
neutral  <- c("8% (764)")
positive  <- c("66% (6,151)")
# negative  <- c("26%")
# neutral  <- c("8%")
# positive  <- c("66%")
# Tensorflow environment ####
library(reticulate)
use_condaenv(condaenv = "env_name",required = T)
library(keras)
library(tensorflow)
tf$constant("Hello Tensorflow!")
library(tidymodels)
library(tidytext)
library(textrecipes)
source("Scripts/EDA.R")
set.seed(1234)
tweets.split <- Tweets %>%
filter(nchar(text)>=15) %>%
initial_split()
tweets.train <- training(tweets.split)
tweets.test <- training(tweets.split)
maxwords<-5000
maxlength<-100
tw_recipe <- recipe(~text, data = tweets.train) %>%
step_tokenize(text) %>%
step_tokenfilter(text, max_tokens = maxwords) %>%
step_sequence_onehot(text, sequence_length = maxlength)
tw_prep <- prep(tw_recipe)
tw_train <- bake(tw_prep, new_data=NULL, composition='matrix')
set.seed(1254)
final_mod <- keras_model_sequential() %>%
layer_embedding(input_dim = maxwords + 1, output_dim = 32) %>%
bidirectional(layer_lstm(
units = 32, dropout = 0.4, recurrent_dropout = 0.4,
return_sequences = TRUE
)) %>%
bidirectional(layer_lstm(
units = 32, dropout = 0.4, recurrent_dropout = 0.4,
return_sequences = TRUE
)) %>%
bidirectional(layer_lstm(
units = 32, dropout = 0.4, recurrent_dropout = 0.4
)) %>%
layer_dense(units = 1, activation = "sigmoid")
final_mod %>%
compile(
optimizer = "adam",
loss = "binary_crossentropy",
metrics = c("accuracy")
)
lstm.history <- final_mod %>%
fit(
tw_train,
tweets.train$score,
epochs = 10,
validation_split = 0.25,
batch_size = 512,
verbose = FALSE
)
lstm.plot <- plot(lstm.history)
lstm.plot
keras_predict <- function(model, baked_data, response) {
predictions <- predict(model, baked_data)[, 1]
tibble(
.pred_1 = predictions,
.pred_class = if_else(.pred_1 < 0.5, 0, 1),
state = response
) %>%
mutate(across(c(state, .pred_class),            ## create factors
~ factor(.x, levels = c(1, 0))))  ## with matching levels
}
tw_test <- bake(tw_prep, new_data = tweets.test, composition = "matrix")
final_res <- keras_predict(final_mod, tw_test, tweets.test$score)
final_metrics <- final_res %>%
metrics(state,  .pred_class, .pred_1)
final_metrics
roc.curve <- final_res %>%
roc_curve(state, .pred_1) %>%
autoplot()
final_res %>%
roc_curve(state, .pred_1) %>%
autoplot()
final_res %>%
conf_mat(state, .pred_class) %>%
autoplot(type = "heatmap")
# Tensorflow environment ####
library(reticulate)
use_condaenv(condaenv = "env_name",required = T)
library(keras)
library(tensorflow)
tf$constant("Hello Tensorflow!")
library(tidymodels)
library(tidytext)
library(textrecipes)
# Load the EDA script. It contains the dataset
source("Scripts/EDA.R")
# LSTM model ####
# set.seed(1234)
tweets.split <- Tweets %>%
filter(nchar(text)>=15) %>%
initial_split()
tweets.train <- training(tweets.split)
tweets.test <- training(tweets.split)
maxwords<-5000
maxlength<-100
tw_recipe <- recipe(~text, data = tweets.train) %>%
step_tokenize(text) %>%
step_tokenfilter(text, max_tokens = maxwords) %>%
step_sequence_onehot(text, sequence_length = maxlength)
tw_prep <- prep(tw_recipe)
tw_train <- bake(tw_prep, new_data=NULL, composition='matrix')
# Keras Model ####
# Create an LSTM model with stack 3 Bidirectional LSTM layers
# Bidirectional layers allows the LSTM network to have both forward and backward information on the sequences on each step
# set.seed(2354)
final_mod <- keras_model_sequential() %>%
layer_embedding(input_dim = maxwords + 1, output_dim = 32) %>%
bidirectional(layer_lstm(
units = 32, dropout = 0.4, recurrent_dropout = 0.4,
return_sequences = TRUE
)) %>%
bidirectional(layer_lstm(
units = 32, dropout = 0.4, recurrent_dropout = 0.4,
return_sequences = TRUE
)) %>%
bidirectional(layer_lstm(
units = 32, dropout = 0.4, recurrent_dropout = 0.4
)) %>%
layer_dense(units = 1, activation = "sigmoid")
# Compile the model
final_mod %>%
compile(
optimizer = "adam",
loss = "binary_crossentropy",
metrics = c("accuracy")
)
# Fit the keras model to the training data
lstm.history <- final_mod %>%
fit(
tw_train,
tweets.train$score,
epochs = 10,
validation_split = 0.25,
batch_size = 512,
verbose = FALSE
)
# Model results
lstm.plot <- plot(lstm.history)
lstm.plot
# Create a function keras_predict
keras_predict <- function(model, baked_data, response) {
predictions <- predict(model, baked_data)[, 1]
tibble(
.pred_1 = predictions,
.pred_class = if_else(.pred_1 < 0.5, 0, 1),
state = response
) %>%
mutate(across(c(state, .pred_class),            ## create factors
~ factor(.x, levels = c(1, 0))))  ## with matching levels
}
# # Test the model using test data
tw_test <- bake(tw_prep, new_data = tweets.test, composition = "matrix")
final_res <- keras_predict(final_mod, tw_test, tweets.test$score)
final_metrics <- final_res %>%
metrics(state,  .pred_class, .pred_1)
final_metrics
final_res %>%
conf_mat(state, .pred_class) %>%
autoplot(type = "heatmap")
# References ####
# This dataset was published in Saif M. Mohammad and Peter Turney. (2013), ``Crowdsourcing a Word-Emotion Association Lexicon.'' Computational Intelligence, 29(3): 436-465.
# https://medium.com/deeplearningmadeeasy/negative-log-likelihood-6bd79b55d8b6
# https://stackoverflow.com/questions/62509324/error-loading-the-keras-package-in-r-studio
final_res %>%
conf_mat(state, .pred_class) %>%
autoplot()
final_res %>%
conf_mat(state, .pred_1) %>%
autoplot(type = "heatmap")
final_res %>%
conf_mat(state, .pred_1) %>%
autoplot()
final_res %>%
roc_curve(state, .pred_1) %>%
autoplot()
runApp()
runApp()
shiny::runApp()
runApp()
install.packages('tufte')
shiny::runApp()
runApp()
dim(Data)
Data <- Data %>%
mutate(Hashtags = str_remove_all(Data$hashtags, "[^ \\w+]"))
Hashtags <- str_remove_all(Data$hashtags, "[^ \\w+]") |>
str_split(" ") |> unlist() |> table()
# convert it to a dataframe
Hashtags <- as.data.frame(Hashtags)
Hashtags[Hashtags == ''] <- NA
Hashtags <- Hashtags %>%
na.omit() %>%
rename(Hashtag = Var1,
Count  = Freq)%>%
filter(Count>2)
Create a wordcloud
wordcloudHashtags <- ggplot(data = Hashtags,
aes(label = Hashtag, size = Count, col = as.character(Count))) +
geom_text_wordcloud(rm_outside = TRUE, max_steps = 1,
grid_size = 1, eccentricity = .9)+
scale_size_area(max_size = 13)+
labs(title = "World cloud of common hashtags")+
theme_minimal()
library(ggwordcloud)
wordcloudHashtags <- ggplot(data = Hashtags,
aes(label = Hashtag, size = Count, col = as.character(Count))) +
geom_text_wordcloud(rm_outside = TRUE, max_steps = 1,
grid_size = 1, eccentricity = .9)+
scale_size_area(max_size = 13)+
labs(title = "World cloud of common hashtags")+
theme_minimal()
# scale_color_brewer(palette = "Paired", direction = -1)+
wordcloudHashtags
wordcloudHashtags
Hashtags <- str_remove_all(Data$hashtags, "[^ \\w+]") |>
str_split(" ") |> unlist() |> table()
Hashtags <- as.data.frame(Hashtags)
Hashtags[Hashtags == ''] <- NA
Hashtags <- Hashtags %>%
na.omit() %>%
rename(Hashtag = Var1,
Count  = Freq)%>%
filter(Count>2)
wordcloudHashtags <- ggplot(data = Hashtags,
aes(label = Hashtag, size = Count, col = as.character(Count))) +
geom_text_wordcloud(rm_outside = TRUE, max_steps = 1,
grid_size = 1, eccentricity = .9)+
scale_size_area(max_size = 13)+
labs(title = "World cloud of common hashtags")+
theme_minimal()
# scale_color_brewer(palette = "Paired", direction = -1)+
wordcloudHashtags
unigram.df <- unigram(DataFrame = TidyTweets)
bigram.df <- bigram(DataFrame = TidyTweets)
unigram.df <- unigram.df %>%
filter(n>50)
unigram.wordcloud <- ggplot(data = unigram.df,
aes(label = word, size = n, col = as.character(n))) +
geom_text_wordcloud(rm_outside = TRUE, max_steps = 1,
grid_size = 1, eccentricity = .9)+
scale_size_area(max_size = 13)+
labs(title = "World cloud of unigrams")+
# scale_color_brewer(palette = "Paired", direction = -1)+
theme_minimal()
unigram.wordcloud
bigram.network <- bigram_network(bigram.df, edge_color = "#00B39A" , node_color = "#F05F1D", set_seed = 1234, layout = "fr", number = 70)
bigram.network
par(mfrow=c(3,1))
par(mfrow=c(3,1))
par(mfrow=c(3,1))
wordcloudHashtags
unigram.wordcloud
bigram.network
install.packages("ggpubr")
library(ggpubr)
combinedPlot <- ggarrange(wordcloudHashtags, unigram.wordcloud, bigram.network, nrow = 3, ncol = 1)
combinedPlot
