# Tensorflow environment ####
library(tensorflow)
use_condaenv(condaenv = "env_name",required = T)
library(reticulate)
tf$constant("Hello Tensorflow!")

# Libraries ####
library(tidyverse)
library(splitstackshape) 
library(tm)
library(tidymodels)
library(tidytext)
library(textrecipes)
library(ggwordcloud)
library(textdata)
library(saotd)
library(syuzhet)
library(stringr)

# Data ####
Data <- read.csv("Data/Tweets2.csv")
RawCopy <- Data

# Data wrangling ####

# Split the column user_location to Location
Data <- cSplit(Data, 'user_location', sep=",", type.convert=FALSE)

# Extract frequency of hashtags
Hashtags <- str_remove_all(Data$hashtags, "[^ \\w+]") |> 
  str_split(" ") |> unlist() |> table()

# convert it to a dataframe
Hashtags <- as.data.frame(Hashtags)

# Replace empty strings with NA
Hashtags[Hashtags == ''] <- NA

# Remove all NA's and rename the columns
Hashtags <- Hashtags %>%
  na.omit() %>%
  rename(Hashtag = Var1, 
         Count  = Freq) %>%
  filter(Count>8)

# Create a wordcloud
wordcloudHashtags <- ggplot(data = Hashtags, 
       aes(label = Hashtag, size = Count, col = as.character(Count))) + 
  geom_text_wordcloud(rm_outside = TRUE, max_steps = 1,
                      grid_size = 1, eccentricity = .9)+
  scale_size_area(max_size = 13)+
  scale_color_brewer(palette = "Paired", direction = -1)+
  theme_void()
wordcloudHashtags


# Remove emoticons, punctuation marks, and stopwords from the dataframe
TidyTweets <- saotd::tweet_tidy(DataFrame = Data)

# Create un-igrams, bi-grams and tri-grams
unigram.df <- unigram(DataFrame = TidyTweets)
bigram.df <- bigram(DataFrame = TidyTweets)
trigram.df <- trigram(DataFrame = TidyTweets)


# Create a unigram wordcloud
unigram.wordcloud <- ggplot(data = unigram.df, 
                            aes(label = word, size = n, col = as.character(n))) + 
  geom_text_wordcloud(rm_outside = TRUE, max_steps = 1,
                      grid_size = 1, eccentricity = .9)+
  scale_size_area(max_size = 13)+
  scale_color_brewer(palette = "Paired", direction = -1)+
  theme_void()
unigram.wordcloud


# Network diagram of bigrams
bigram.network <- bigram_network(bigram.df, node_color = "red", set_seed = 1234, layout = "star", number = 70)
bigram.network

# Derive sentiment classifiers
location.senti.score <- data_frame(id=TidyTweets$user_location_1, text = TidyTweets$text) %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  inner_join(get_sentiments("nrc")) %>%
  mutate(score = ifelse(sentiment=='positive',1,
                        ifelse(sentiment=='joy',1,
                               ifelse(sentiment=='anticipation',1,
                                      ifelse(sentiment=='trust',1,
                                             ifelse(sentiment=='surprise',1,-1)))))) %>%
  group_by(id) %>%
  summarise(total_score = sum(score)) %>%
  mutate(sentiment = ifelse(total_score>0,'positive',ifelse(total_score<0,'negative','neutral')))

View(location.senti.score)

# Columnplot of location-based tweets based on the sentimentscore



# Overall sentiscore
TidyTweets <- tibble::rowid_to_column(TidyTweets, "id")
senti.score <- data_frame(id=TidyTweets$id, text = TidyTweets$text) %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  inner_join(get_sentiments("nrc")) %>%
  mutate(score = ifelse(sentiment=='positive',1,ifelse(sentiment=='joy',1,ifelse(sentiment=='anticipation',1,ifelse(sentiment=='trust',1,ifelse(sentiment=='surprise',1,-1)))))) %>%
  group_by(id) %>%
  summarise(total_score = sum(score)) %>%
  mutate(sentiment = ifelse(total_score>0,'positive',ifelse(total_score<0,'negative','neutral')))

# get the dataframe which contains tweet message, id and it's sentiment
senti.score <- TidyTweets %>% inner_join(senti.score, by='id') %>% select('id', 'text','sentiment')

# Recode the response variable to integers
Tweets <- senti.score %>%
  mutate(score = recode(sentiment, 'negative'=-1, 'neutral'=0, 'positive'=1))


# LSTM model ####
set.seed(1234)
tweets.split <- Tweets %>%
  filter(nchar(text)>=15) %>%
  initial_split()

tweets.train <- training(tweets.split)
tweets.test <- training(tweets.split)


maxwords<-5000
maxlength<-100

tw_recipe <- recipe(~text, data = tweets.train) %>%
  step_tokenize(text) %>%
  step_tokenfilter(text, max_tokens = maxwords) %>%
  step_sequence_onehot(text, sequence_length = maxlength)


tw_prep <- prep(tw_recipe)
tw_train <- bake(tw_prep, new_data=NULL, composition='matrix')

dim(tw_train)

# Keras sequential model
lstm_mod <- keras_model_sequential() %>%
  layer_embedding(input_dim = maxwords + 1, output_dim = 32) %>%
  layer_lstm(units = 32) %>%
  layer_dense(units = 1, activation = "sigmoid")

lstm_mod %>%
  compile(
    optimizer = "adam",
    loss = "binary_crossentropy",
    metrics = c("accuracy")
  )

lstm_history <- lstm_mod %>%
  fit(
    tw_train,
    tweets.train$score,
    epochs = 10,
    validation_split = 0.25,
    batch_size = 512,
    verbose = FALSE
  )

plot(lstm_history)








# References ####
# This dataset was published in Saif M. Mohammad and Peter Turney. (2013), ``Crowdsourcing a Word-Emotion Association Lexicon.'' Computational Intelligence, 29(3): 436-465.

